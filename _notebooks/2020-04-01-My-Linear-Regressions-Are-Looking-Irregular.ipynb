{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"My Linear Regressions Are Looking Irregular\"\n",
    "> \"An introductory guide to regularizing linear regressions.\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [linear regression, regularization]\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [General Assembly's *Data Science Immersive*](https://generalassemb.ly/education/data-science-immersive/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to regularize our linear regressions? Overfitting.\n",
    "What is overfitting?\n",
    "- Overfitting means building a model that matches the training data \"too closely.\"\n",
    "- The model ends up fitting to noise rather than signal.\n",
    "- The **bias is too low** and the **variance is too high**.\n",
    "\n",
    "What can cause overfitting?\n",
    "- Irrelevant features are included in the model.\n",
    "- The number of features is close to the number of observations.\n",
    "- The features are correlated to each other.\n",
    "- The coefficients of the features are large.\n",
    "What is regularization and how does it help?\n",
    "- Regularization helps against overfitting by imposing a penalty that decreases the coefficients of the features.\n",
    "- Regularization lowers the fit for the training data, but it improves the fit for the test data.\n",
    "\n",
    "### The two most common types of regularization for linear regressions are *Lasso* and *Ridge*. *Elastic Net* is a mixture of the two.\n",
    "- Lasso uses the L1 regularization method, Ridge uses L2, and Elastic Net uses both.\n",
    "- We will be regularizing linear regressions in this post, but other types of regressions can be regularized with L1 and L2 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What kind of situations would call for a regularized linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predicting home prices  based on number of bedrooms upstairs, number of bedrooms downstairs, and total number of bedrooms.\n",
    "- Predicting a wine's rating based on its fixed acidity, volatile acidity, citric acid, residual sugar, chloride, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and percent alcohol.\n",
    "- Predicting the ratio of scores for the teams in a basketball game based on the player stats and the team stats.\n",
    "- Estimating a country's life expectancy based on gross domestic product per person, infant mortality, and region of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### How to run the Ridge, Lasso, and Elastic Net regressions in Python, using example values.\n",
    "\n",
    "### *Preparation for Running the Regressions*\n",
    "- Start by importing the Pandas and NumPy libraries, in order to create DataFrames and arrays, as well as the different types of linear regressions we will use, Ridge, Lasso, and Elastic Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store your training dataset's features in a Pandas DataFrame as X\n",
    "- Store your training dataset's target variable in an array as y.\n",
    "- This is the R^2 score from **cross-validation** for a linear regression for the sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "cross_val_score(lr_model, X_overfit, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.1913277098938003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The R^2 score is very low. Is the model overfitted, and will regularization improve it?\n",
    "\n",
    "### *Running a Ridge Regression*\n",
    "- Ridge regression is **more computationally efficient than Lasso**.\n",
    "- Ridge regression usually uses larger $\\alpha$ (values for the penalty) than the Lasso regression.\n",
    "\n",
    "Pick a list of $\\alpha$-values, instantiate the model, and fit the regression. Save the optimal model and check its R^2 score from cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.logspace(0, 5, 200)\n",
    "ridge_model = RidgeCV(alphas = alpha_list, store_cv_values = True) # store_cv_values picks best alpha value\n",
    "ridge_model = ridge_model.fit(X_overfit, y)\n",
    "ridge_model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 821.434358491943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_opt_model = Ridge(alpha = ridge_model.alpha_)\n",
    "cross_val_score(ridge_opt_model, X_overfit, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.2221069221200395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This R^2 score is an improvement over the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Running a Lasso Regression*\n",
    "- If group of predictors are highly correlated, the Lasso regression picks only one of them and shrinks the others to zero.\n",
    "- The Lasso regression is better with smaller $\\alpha$ (values for the penalty) than the Ridge regression.\n",
    "\n",
    "Pick a list of $\\alpha$-values, instantiate the model, and fit the regression. Save the optimal model and check its R^2 score from cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_alpha_list = np.arange(0.001, 0.15, 0.0025)\n",
    "lasso_model = LassoCV(alphas = l_alpha_list, cv = 5)\n",
    "lasso_model = lasso_model.fit(X_overfit, y)\n",
    "lasso_model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha = lasso_model.alpha_)\n",
    "cross_val_score(lasso_model, X_overfit, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.2617417795359766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This R^2 score is an improvement over the Ridge regression.\n",
    "\n",
    "### *Running an Elastic Net Regression*\n",
    "- The Elastic Net regression adds a larger penalty than the Ridge and Lasso regressions.\n",
    "- Elastic Net should only be used if the training accuracy is much higher than the test accuracy or if the independent variables are highly correlated.\n",
    "\n",
    "Pick a list of $\\alpha$-values, instantiate the model, and fit the regression. Save the optimal model and check its R^2 score from cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_alphas = np.arange(0.5, 1.0, 0.005)\n",
    "enet_model = ElasticNetCV(alphas = enet_alphas)\n",
    "enet_model = enet_model.fit(X_overfit, y)\n",
    "enet_model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_model = ElasticNet(alpha = enet_model.alpha_)\n",
    "cross_val_score(enet_model, X_overfit, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0.07522288223865307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This R^2 score is worse than the linear regression, the Ridge regression, and the Lasso regression, so this model penalized our feature coefficients too much.\n",
    "\n",
    "### *Conclusions from the Code*\n",
    "- For this dataset, the best model was Lasso regression. Compare all of the models with ranges of parameters to determine the best-fitting model.\n",
    "- Don't forget to perform feature engineering before modeling, and to re-optimize your model if you go back and engineer features. Feature engineering will affect the optimal model and its optimal parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
