{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"A Caption Generator Full of Curiosities\"\n",
    "> \"A creative caption generator based on limited and varied training data.\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [natural language processing, caption generation, LSTM neural network]\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "### What does a neural-network-based caption generator do?\n",
    "- A caption generator takes a photograph and outputs a caption. A caption generator trained on a neural network knows what kind of words to output based on previous captions and photographs that it has been trained on.\n",
    "- All of the neural-network-based caption generator projects I found used large datasets of images with descriptive captions (e.g., the Flickr8k dataset, containing 8,000 images with 5 captions each). The three best example projects that I referenced when creating my project are listed below:\n",
    "    - [Image Captioning with Keras](https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8) by Harshall Lamba\n",
    "    - [How to Develop a Deep Learning Photo Caption Generator from Scratch](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/) by Dr. Jason Brownlee\n",
    "    - [Python based Project â€“ Learn to Build Image Caption Generator with CNN & LSTM](https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/) by the Dataflair Team\n",
    "- The goal of these caption generators was to describe the contents of the images precisely, not to develop creative captions.\n",
    "\n",
    "### What was the purpose of my caption generator?\n",
    "- I created a caption generator that was trained on travel photographs from the \"Places\" category on the website [Atlas Obscura](https://www.atlasobscura.com/places) in order to help people write creative travel captions by capturing the mood of the photographs. My secondary goal was to explore the uses and limits of neural networks.\n",
    "- I described in [my last blog post](https://ynusinovich.github.io/Data-Science-Journeys/web%20scraping/scrapy/xpath%20and%20css%20selectors/2020/05/04/The-Itsy-Bitsy-Spider-Climbed-into-Your-Website.html) how I used Scrapy to download the image links and image captions from Atlas Obscura.\n",
    "- Due to the large variations in captions on Atlas Obscura, which were meant to be historical and artistic and not just descriptive, I did not expect the model to generate perfect captions. [A similar type of project](https://towardsdatascience.com/do-it-for-the-gram-instagram-style-caption-generator-4e7044766e34) for generating Instagram captions ended up creating repeating captions for most photographs.\n",
    "- The first time fitting my caption generator model gave me the following odd caption for every image I tested: \"the worlds largest bioluminescent salt mine is the most remote place in the world and the worlds largest collection of curiosities and curiosities and curiosities and curiosities and curiosities and curiosities and curiosities and curiosities and curiosities and curiosities,\" which is the inspiration for the name of this blog post.\n",
    "\n",
    "### What is a neural network?\n",
    "As explained in [this article from MIT News](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414), \"Neural net\\[work\\]s are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples. Usually, the examples have been hand-labeled in advance. An object recognition system, for instance, might be fed thousands of labeled images of cars, houses, coffee cups, and so on, and it would find visual patterns in the images that consistently correlate with particular labels.\"\n",
    "\n",
    "## Examples\n",
    "### What would a *travel* caption generator be good for?\n",
    "- Instagram, Facebook, or Pinterest captions\n",
    "- Writing travel articles\n",
    "- Advertising travel destinations\n",
    "\n",
    "### What could *other* caption generators be good for?\n",
    "- Restaurant menus\n",
    "- Real estate brochures\n",
    "- Furniture descriptions\n",
    "\n",
    "## Code Part 1\n",
    "### How to create a travel photograph caption generator.\n",
    "To start, complete the necessary imports. The command at the end resolved memory issues that stopped the model from running, but you may not need to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM, Bidirectional, Embedding\n",
    "from keras.layers.merge import add\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the caption dictionary and remove punctuation and numbers from the captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary on the first run\n",
    "\n",
    "caption_df = pd.read_csv(\"../Data/atlas_edits_clean.csv\")\n",
    "caption_df.rename(columns = {\"Unnamed: 0\": \"image_number\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary on the first run\n",
    "\n",
    "caption_dict = dict()\n",
    "for i in range(0,len(caption_df.index)):\n",
    "    caption_dict[caption_df.loc[i, \"image_number\"]] = caption_df.loc[i, \"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary on the first run\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "# characters to replace, characters to replace them with, characters to delete\n",
    "\n",
    "for image_number, description in caption_dict.items():\n",
    "    # tokenize\n",
    "    description = description.split()\n",
    "    # convert to lower case\n",
    "    description = [word.lower() for word in description]\n",
    "    # remove punctuation from each token\n",
    "    description = [word.translate(table) for word in description]\n",
    "    # remove hanging 's' and 'a'\n",
    "    description = [word for word in description if len(word) > 1]\n",
    "    # remove tokens with numbers in them\n",
    "    description = [word for word in description if word.isalpha()]\n",
    "    # store as string\n",
    "    description = ' '.join(description)\n",
    "    # save in dict\n",
    "    caption_dict[image_number] =  description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary on the first run\n",
    "\n",
    "# Create a list of all the training captions\n",
    "all_captions = []\n",
    "for image_number, description in caption_dict.items():\n",
    "    all_captions.append(description)\n",
    "\n",
    "# Consider only words which occur at least 3 times in the corpus\n",
    "word_count_threshold = 3\n",
    "word_counts = dict()\n",
    "num_sentences = 0\n",
    "for sent in all_captions:\n",
    "    num_sentences += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1 # add one to the count of the word\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print(f'Preprocessed words {len(vocab)} ')\n",
    "print(f'Number of sentences {num_sentences} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary on the first run\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('../Obj/' + name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(vocab, \"vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On subsequent runs of your code, the vocabulary can be loaded from the folder you saved it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the vocabulary on subsequent runs\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('../Obj/' + name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "vocab = load_obj(\"vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 to the vocabulary, to account for blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add \"startseq\" to the beginning of each sequence and \"endseq\" to the end, so the model learns how to start and end a caption, and then save the finalized caption dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the caption dictionary on the first run\n",
    "\n",
    "for image_number, description in caption_dict.items():\n",
    "    tokens = description.split()\n",
    "    caption_dict[image_number] = 'startseq ' + ' '.join(tokens) + ' endseq'\n",
    "    print(f\"Image {image_number} added to caption dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the caption dictionary on the first run\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('../Obj/' + name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(caption_dict, \"caption_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On subsequent runs of your code, the caption dictionary can be loaded from the folder you saved it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the caption dictionary on subsequent runs:\n",
    "    \n",
    "caption_dict = load_obj(\"caption_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the images through both the pre-processing step and the processing step of [Google's InceptionV3 image processing model](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202). Remove the last two layers of the model so that the output would be a processed image data vector. The last two layers would have taken the processed image vector and outputted image classes based on images that the model was trained on. I saved after each step, but you could also complete both pre-processing and processing and then save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yannusinovich/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yannusinovich/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights = 'imagenet')\n",
    "# Remove the last layer (output softmax layer) from the InceptionV3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image dictionaries on the first run\n",
    "\n",
    "pre_image_dict = dict()\n",
    "\n",
    "for filename in os.listdir(\"../Data/Atlas_Images/\"):\n",
    "    if filename.endswith(\".jpg\"): \n",
    "        image_path = os.path.join(\"../Data/Atlas_Images/\", filename)\n",
    "        # Convert all the images to size 299 x 299 as expected by the InceptionV3 Model\n",
    "        img = image.load_img(image_path, target_size = (299, 299))\n",
    "        # Convert PIL image to numpy array of 3-dimensions\n",
    "        x = image.img_to_array(img)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis = 0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        # add to pre-imagedict\n",
    "        image_number = int(filename.split(\".\")[0])\n",
    "        pre_image_dict[image_number] = x\n",
    "        print(f\"Image {image_number} added to pre-image dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image dictionaries on the first run\n",
    "\n",
    "save_obj(pre_image_dict, \"pre_image_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image dictionaries on the first run\n",
    "\n",
    "image_dict = dict()\n",
    "\n",
    "for filename in os.listdir(\"../Data/Atlas_Images/\"):\n",
    "    if filename.endswith(\".jpg\"): \n",
    "        image_path = os.path.join(\"../Data/Atlas_Images/\", filename)\n",
    "        image_number = int(filename.split(\".\")[0])\n",
    "        pre_image_dict[image_number] = x\n",
    "        feature = model_new.predict(x, verbose = 0)\n",
    "        feature = np.reshape(feature, feature.shape[1])\n",
    "        image_dict[image_number] = feature\n",
    "        print(f\"Image {image_number} added to image dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image dictionaries on the first run\n",
    "\n",
    "save_obj(image_dict, \"image_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the only reason I saved the pre-image dictionary was that my computer crashed between the pre-processing step and the processing step. If yours doesn't, then don't save the pre-image dictionary - it's quite large.\n",
    "\n",
    "On subsequent runs of your code, the image dictionary can be loaded from the folder you saved it to. You do not need to re-load the pre-image dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image dictionary on subsequent runs\n",
    "    \n",
    "image_dict = load_obj(\"image_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an index to encode words to numbers and back to words. Additionally, calculate the maximum length of the captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "index = 1\n",
    "for word in vocab:\n",
    "    word_to_index[word] = index\n",
    "    index_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Caption Length, in Words: 40\n"
     ]
    }
   ],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(a_caption_dict):\n",
    "    all_desc = list()\n",
    "    for key in a_caption_dict.keys():\n",
    "        all_desc.append(a_caption_dict[key])\n",
    "    return all_desc\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(a_caption_dict):\n",
    "    lines = to_lines(a_caption_dict)\n",
    "    return max(len(caption.split()) for caption in lines)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(caption_dict)\n",
    "print(f'Max Caption Length, in Words: {max_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a caption generator function. The caption generator continuously takes images and captions and yields data for the model to fit. The images are in the form of arrays. The captions are in the form of arrays of input word sequences, padded with zeros to be the same length, and words encoded categorically based on the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(a_caption_dict, an_image_dict, a_word_to_index, a_max_length, a_num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    # loop forever over images\n",
    "    while 1:\n",
    "        for image_number, caption in a_caption_dict.items():\n",
    "            n += 1\n",
    "            # retrieve the photo features\n",
    "            photo_data = an_image_dict[image_number]\n",
    "            # encode the sequence\n",
    "            seq = [a_word_to_index[word] for word in caption.split(' ') if word in a_word_to_index]\n",
    "            # split one sequence into multiple X, y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen = a_max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photo_data)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n == a_num_photos_per_batch:\n",
    "                yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding matrix using the pre-trained [GloVe vectors](https://nlp.stanford.edu/projects/glove/) to be able to embed the caption words as vectors based on their similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe vectors\n",
    "glove_dir = '../Data/'\n",
    "embeddings_index = dict()\n",
    "file = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding = \"utf-8\")\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "# Get 200-dim dense matrix for each of the words in our vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network model to process the image and caption data. The image extractor uses a regular dense layer with dropout for regularization, and the caption sequencer uses an embedding layer, dropout for regularization, and a [long short term memory](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  network (LSTM) to remember words from earlier in the sequence. Finally, both outputs are sent to two more dense layers, the first of which has [L2 regularization](https://ynusinovich.github.io/Data-Science-Journeys/linear%20regression/regularization/2020/04/01/My-Linear-Regressions-Are-Looking-Irregular.html). The weights of the embedding layer are fixed so that the fitting of the model does not change the pre-trained vectors of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yannusinovich/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input(shape = (2048,))\n",
    "fe1 = Dropout(0.05)(inputs1)\n",
    "fe2 = Dense(256, activation = 'relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape = (max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero = True)(inputs2)\n",
    "se2 = Dropout(0.1)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(0.005))(decoder1)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs = [inputs1, inputs2], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 200)      1609400     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8047)         2068079     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,735,783\n",
      "Trainable params: 3,126,383\n",
      "Non-trainable params: 1,609,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a `learning_rate`. [This blog post](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/) is useful guidance when setting the `learning_rate`. Use `categorical_crossentropy` for the `loss`, since each output word corresponds to one of the words (categories) in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate = 0.005)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the model and then save it. The `num_photos_per_batch` can be reduced if you need to use less memory, and you can add `EarlyStopping` so that the model will stop training if `loss` is not decreasing for a `patience` of 5 epochs. The `steps_per_epoch` below are the default from the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_photos_per_batch = 16\n",
    "\n",
    "generator = data_generator(caption_dict, image_dict, word_to_index, max_length, num_photos_per_batch)\n",
    "\n",
    "early_stop = EarlyStopping(monitor = \"loss\", \n",
    "                           patience = 5,\n",
    "                           min_delta = 0,\n",
    "                           restore_best_weights = True)\n",
    "\n",
    "model.fit_generator(generator, epochs = 100, verbose = 1,\n",
    "                    steps_per_epoch = math.ceil(len(caption_dict)/num_photos_per_batch),\n",
    "                    callbacks = [early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../Obj/final_model_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Part 2\n",
    "### How to output captions with your travel photograph caption generator.\n",
    "Start with the same steps as in the previous section. First, load up the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open('../Obj/' + name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "vocab = load_obj(\"vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "index = 1\n",
    "for word in vocab:\n",
    "    word_to_index[word] = index\n",
    "    index_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, process the images in your test folder using the InceptionV3 model, and load the trained caption generator model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = InceptionV3(weights = 'imagenet')\n",
    "new_model = Model(pre_model.input, pre_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = dict()\n",
    "\n",
    "for filename in os.listdir(\"../Data/Test_Photos/\"):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"): \n",
    "        image_path = os.path.join(\"../Data/Test_Photos/\", filename)\n",
    "        # Convert all the images to size 299 x 299 as expected by the InceptionV3 Model\n",
    "        img = image.load_img(image_path, target_size = (299, 299))\n",
    "        # Convert PIL image to numpy array of 3-dimensions\n",
    "        x = image.img_to_array(img)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis = 0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        feature = new_model.predict(x, verbose = 0)\n",
    "        feature = np.reshape(feature, feature.shape[1])\n",
    "        image_number = int(filename.split(\".\")[0])\n",
    "        image_dict[image_number] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../Obj/final_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a function to generate captions. Given a photograph processed through the steps above, the function chooses one of the top `a_randomness` word indices predicted by the model, converts it back to a word, and adds it to the output sequence. The next word in the output sequence is predicted by the photograph and the previous words. Using the top `a_randomness` words, instead of restricting the caption generator function to the top single word, introduces some randomness to prevent captions from looking too similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captionGenerator(a_photo, a_randomness):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen = max_length)\n",
    "        yhat = model.predict([a_photo, sequence], verbose = 0)\n",
    "        yhat = np.random.choice((-yhat).argsort(axis = -1)[:, :a_randomness][0])\n",
    "        word = index_to_word[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this brief example only prints the caption for the first photo in the folder\n",
    "\n",
    "photo = image_dict[0].reshape(1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captionGenerator(photo, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Captions for an Image\n",
    "I used this sample photograph of Kotor, Montenegro, from the [NBC News](https://media4.s-nbcnews.com/i/newscms/2015_44/839916/lonely-planet-travel-kotor-today-151029-tease_84543027bb82cc77d65d99229ec6cac0.jpg) website:\n",
    "\n",
    "![](2020-05-15-Image-1.jpeg)\n",
    "\n",
    "Running the caption generator resulted in captions like:\n",
    "- \"the largest manmade crater on the earth and and an ancient egyptian village in an extreme world of the world is now home to the most important collection with oddities and oddities specimens and antique penny of dentistry and\"\n",
    "- \"of the world and most powerful of the world and an ancient church with the largest collection of the world and most important and curiosities of pathological planetariums and and the largest planetariums of the world and most curious\"\n",
    "- \"and the worlds tallest cave in north america the world is the worlds tallest hole in europe is the largest drain in europe of the world is now an amazing illusion and an eccentric illusion of the illustrious achievements\"\n",
    "\n",
    "Powerful, I couldn't have captioned it better myself.\n",
    "\n",
    "Feel free to play around with a smaller version of the caption generator (that would fit on the free version of Heroku) in [my web app](https://caption-generator-obscura.herokuapp.com/). Keep in mind that on the web app, I refer to the \"randomness\" as \"unpredictability.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
